### k-NN 알고리즘(k-Nearest Neighbors)
"""
- 예측하려는 데이터에 대해 거리(유사성)가 가장 가까운 데이터(이웃)를 찾아 예측에 사용하는 알고리즘
- 둘 이상의 데이터(이웃)를 선택할 때 데이터가 많은 클래스를 기준으로 최종 이웃으로 선택 
- 이해하기 매우 쉽고 매개변수에 대한 복잡한 조정 없이 쉽게 좋은 성능을 발휘
- 학습 데이터의 크기가 너무 크면(변수의 개수, 데이터의 개수 등) 처리 속도가 느려짐
- 데이터를 구성하는 값이 대부분 0인 경우 잘 동작하지 않음
- 데이터 간 거리를 측정할 때 값의 범위(단위)가 다를 경우 범위가 작은 데이터에 영향을 크게 받기 때문에 단위를 통일시키는 정규화 과정이 필요
- 주요 매개변수
    - metirc & p : 데이터 간 거리를 측정하는 방식, 기본값 = minkowski & 2 = euclidian (여러 환경에서 잘 동작하여 일반적으로 사용됨)
    - n_neighbors : 이웃의 개수
- 결정경계(decision boundary) 
    - 클래스들 간의 영역을 구분하는 경계
    - 이웃(n_neighbors)의 수가 증가할 수록 부드러운 결정 경계 생성 => 단순한 모델을 의미
    - 이웃의 수가 작으면 모델의 복잡도가 증가
    - 일반적으로 모델의 복잡도가 높아질 수록 학습 데이터에 대한 정확도는 높아지지만 새로운 데이터에 일반화하는 성능은 낮아지는 경향
    - 이웃의 수를 증가시키면 학습 데이터에 대학 정확도는 낮아지더라도 테스트 데이터에 대한 정확도가 향상되며 안정적인 예측값을 얻을 수 있음
    - 이웃의 수가 많다고 해서 무조건 테스트 데이터에 대한 정확도가 높은 것은 아님
"""
#쉬운데 성능도 좋다! but 데이터 많으면 성능이 떨어진다?
# k - means clustering 과의 차이점은 kmc 는 중심점을 중심으로 했지만, 
# k-nn은 입력데이터와 가까운 데이터와 비교! 

