### k-NN 알고리즘(k-Nearest Neighbors)
"""
- 예측하려는 데이터에 대해 거리(유사성)가 가장 가까운 데이터(이웃)를 찾아 예측에 사용하는 알고리즘
- 둘 이상의 데이터(이웃)를 선택할 때 데이터가 많은 클래스를 기준으로 최종 이웃으로 선택 
- 이해하기 매우 쉽고 매개변수에 대한 복잡한 조정 없이 쉽게 좋은 성능을 발휘
- 학습 데이터의 크기가 너무 크면(변수의 개수, 데이터의 개수 등) 처리 속도가 느려짐
- 데이터를 구성하는 값이 대부분 0인 경우 잘 동작하지 않음
- 데이터 간 거리를 측정할 때 값의 범위(단위)가 다를 경우 범위가 작은 데이터에 영향을 크게 받기 때문에 단위를 통일시키는 정규화 과정이 필요
- 주요 매개변수
    - metirc & p : 데이터 간 거리를 측정하는 방식, 기본값 = minkowski & 2 = euclidian (여러 환경에서 잘 동작하여 일반적으로 사용됨)
    - n_neighbors : 이웃의 개수
- 결정경계(decision boundary) 
    - 클래스들 간의 영역을 구분하는 경계
    - 이웃(n_neighbors)의 수가 증가할 수록 부드러운 결정 경계 생성 => 단순한 모델을 의미
    - 이웃의 수가 작으면 모델의 복잡도가 증가
    - 일반적으로 모델의 복잡도가 높아질 수록 학습 데이터에 대한 정확도는 높아지지만 새로운 데이터에 일반화하는 성능은 낮아지는 경향
    - 이웃의 수를 증가시키면 학습 데이터에 대학 정확도는 낮아지더라도 테스트 데이터에 대한 정확도가 향상되며 안정적인 예측값을 얻을 수 있음
    - 이웃의 수가 많다고 해서 무조건 테스트 데이터에 대한 정확도가 높은 것은 아님 (평균 5~7개를 쓴다)
"""
#쉬운데 성능도 좋다! but 데이터 많으면 성능이 떨어진다?
# k - means clustering 과의 차이점은 kmc 는 중심점을 중심으로 했지만, 
# k-nn은 입력데이터와 가까운 데이터와 비교! 

# knn 예제 이미지
# n_neighbors = 1 : 가장 가까운 1개의 데이터를 이웃으로 선택, 군집화 
mglearn.plots.plot_knn_classification(n_neighbors=1)

# KNN 예제 이미지
# n_neighbors=3 : 가장 가까운 3개의 데이터를 이웃으로 선택, 군집화 

mglearn.plots.plot_knn_classification(n_neighbors=3)

#-------------------------------------------------------------------------------------

# 학습 및 테스트 데이터 생성

from sklearn import datasets
from sklearn.model_selection import train_test_split

# iris 데이터셋 로드
iris = datasets.load_iris()
# data => 문제 (x), target => 정답 (y)
iris_x = iris.data
iris_y = iris.target

# 얻어온 iris_x, iris_y를 train_test_split으로 학습, 시험데이터 분류하기 8:2 비율
train_x, test_x, train_y, test_y = train_test_split(iris_x,iris_y,test_size=.2)

#KNN 라이브러리 로드 
from sklearn.neighbors import KNeighborsClassifier
# KNN 변수 생성
knc = KNeighborsClassifier()
# 모델 학습
knc.fit(train_x,train_y)
# 학습용 데이터에 대한 결과 예측
knc.predict(train_x)
# 학습 데이터에 대한 정확도 계산 
knc.score(train_x,train_y)
# 테스트 데이터에 대한 결과값 
knc.predict(test_x)
# 테스트 데이터에 대한 정확도 계산 
knc.score(test_x,test_y)
# 임의로 이중리스트 형태로 붓꽃 1개의 데이터를 입력해서 
# 결과물을 예측시켜보세요.
random_test_knn = [[5.2,2.1,2.1,3.2],
                  [3.1,0.2,3.2,6.1]]
knc.predict(random_test_knn)
